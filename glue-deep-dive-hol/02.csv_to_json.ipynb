{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV -> JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이번 Lab에서는 Glue Job의 기본 Template을 살펴보고 CSV 파일을 JSON으로 변환하는 Glue Job을 만들어 실행하고 디버깅 하는 과정을 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3에 업로드한 데이터를 읽어오기 위해 각자 S3 bucket에 지정한 account-id를 account_id 변수에 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Read & Write with Spark API\n",
    "#### 아래 코드 실행 후 s3://aws-glue-hol-[account id]/output 디렉토리에 json 파일이 정상적으로 생성되었는 지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# GlueContext 생성\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "s3_bucket = 's3://aws-glue-hol-' + ACCOUNT_ID\n",
    "\n",
    "# Read CSV file using Spark API\n",
    "titanic_csv_df = spark.read.csv(s3_bucket + '/train', header=True)\n",
    "# Create initial Column using Spark API\n",
    "titanic_csv_df = titanic_csv_df.withColumn('initial', regexp_extract(col('Name'), \"(\\w+)\\.\", 1))\n",
    "# Drop Name Column using Glue API\n",
    "titanic_csv_dyf = DynamicFrame.fromDF(titanic_csv_df, glueContext, 'titanic_csv_dyf').drop_fields('Name')\n",
    "\n",
    "# Write JSON file using Spark API\n",
    "titanic_csv_dyf.toDF().write \\\n",
    "    .format('json') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(s3_bucket + '/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Read & Write with Glue API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# GlueContext 생성\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "s3_bucket = 's3://aws-glue-hol-' + ACCOUNT_ID\n",
    "\n",
    "# Read CSV file using Glue API\n",
    "titanic_dyf = glueContext.create_dynamic_frame.from_catalog(database='analytics_hol',\n",
    "                                                           table_name='titanic_train',                           \n",
    "                                                           transformation_ctx='titanic_dyf')\n",
    "\n",
    "# Create initial Column using Spark API\n",
    "titanic_csv_df = titanic_dyf.toDF()\n",
    "titanic_csv_df = titanic_csv_df.withColumn('initial', regexp_extract(col('Name'), \"(\\w+)\\.\", 1))\n",
    "\n",
    "# Drop Name Column using Glue API\n",
    "titanic_csv_dyf = DynamicFrame.fromDF(titanic_csv_df, glueContext, 'titanic_csv_dyf').drop_fields('Name')\n",
    "\n",
    "# Write JSON file using Glue API\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame=titanic_csv_dyf, \n",
    "    connection_type = \"s3\", \n",
    "    connection_options = {\"path\": s3_bucket + '/output'}, \n",
    "    format = \"json\", \n",
    "    transformation_ctx = \"titanic_json_dyf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제 Glue Job을 실습하기 위한 코드입니다.\n",
    "#### Lab Guide를 따라 Glue Console에서 Job을 만들고 아래 코드를 Copy해서 Job을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "# SparkContext 생성\n",
    "sc = SparkContext()\n",
    "# GlueContext 생성\n",
    "glueContext = GlueContext(sc)\n",
    "# SparkSession 생성\n",
    "spark = glueContext.spark_session\n",
    "# Job 생성\n",
    "job = Job(glueContext)\n",
    "# Job 초기화\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "s3_bucket = 's3://aws-glue-hol-' + ACCOUNT_ID\n",
    "\n",
    "# S3에서 csv 데이터를 읽어 DynamicFrame 생성\n",
    "titanic_dyf = glueContext.create_dynamic_frame_from_options(\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths': [s3_bucket + '/train']},\n",
    "    format='csv',\n",
    "    format_options={\n",
    "        \"withHeader\": True,\n",
    "        \"delimiter\": ','\n",
    "    })\n",
    "\n",
    "# Spark 활용: DynamicFrame을 DataFrame으로 변환 및 initail column을 추가\n",
    "titanic_csv_df = titanic_dyf.toDF()\n",
    "titanic_csv_df = titanic_csv_df.withColumn('initial', regexp_extract(col('Name'), \"(\\w+)\\.\", 1))\n",
    "\n",
    "# Glue 활용: DataFrame을 DynamicFrame으로 변환하여 Name column 삭제\n",
    "titanic_csv_dyf = DynamicFrame.fromDF(titanic_csv_df, glueContext, 'titanic_csv_dyf').drop_fields('Name')\n",
    "\n",
    "# json format으로 output 디렉토리에 저장\n",
    "titanic_csv_dyf.toDF().write \\\n",
    "    .format('json') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(s3_bucket + '/output')\n",
    "# Job commit\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
