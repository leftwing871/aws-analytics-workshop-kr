{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지금까지는 Glue 작업 타입 중 Spark 작업을 살펴보았고 이번 Lab에서는 Python Shell 작업을 생성하고 실행하는 과정을 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV -> JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDS의 Table을 Reset하기 위해 HOST, PASSWD 정보를 입력하고 아래 코드 Cell을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = ''\n",
    "USER = 'admin'\n",
    "PASSWD = ''\n",
    "DATABASE = 'analytics_hol'\n",
    "TABLE = 'titanic_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host=HOST, user=USER, passwd=PASSWD, connect_timeout=5)   \n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        # drop db\n",
    "        query = 'DROP DATABASE IF EXISTS {DATABASE}'.format(DATABASE=DATABASE)\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # create db\n",
    "        query = 'CREATE DATABASE IF NOT EXISTS {DATABASE}'.format(DATABASE=DATABASE)\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # create table\n",
    "        query = '''\n",
    "CREATE TABLE IF NOT EXISTS {DATABASE}.{TABLE} (\n",
    "  `passengerid` int(11) NOT NULL,\n",
    "  `survived` tinyint(1) DEFAULT NULL,\n",
    "  `pclass` tinyint(4) DEFAULT NULL,\n",
    "  `name` varchar(128) COLLATE utf8_bin DEFAULT NULL,\n",
    "  `sex` char(8) DEFAULT NULL,\n",
    "  `age` tinyint(4) DEFAULT NULL,\n",
    "  `sibsp` tinyint(4) DEFAULT NULL,\n",
    "  `parch` tinyint(4) DEFAULT NULL,\n",
    "  `ticket` varchar(32) COLLATE utf8_bin DEFAULT NULL,\n",
    "  `fare` DECIMAL(10, 6) DEFAULT NULL,\n",
    "  `cabin` varchar(32) COLLATE utf8_bin DEFAULT NULL,\n",
    "  `embarked` char(1) DEFAULT NULL,\n",
    "  `created_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "  `updated_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "  PRIMARY KEY (`passengerid`),\n",
    "  KEY `survived` (`survived`),\n",
    "  KEY `pclass` (`pclass`),\n",
    "  KEY `sex` (`sex`),\n",
    "  KEY `embarked` (`embarked`),\n",
    "  KEY `created_time` (`created_time`),\n",
    "  KEY `updated_time` (`updated_time`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin\n",
    "        '''.format(DATABASE=DATABASE, TABLE=TABLE)\n",
    "        cursor.execute(query)\n",
    "\n",
    "        conn.commit()\n",
    "except Exception as e:\n",
    "    print('[ERROR]: {}'.format(e))\n",
    "    raise\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write CSV file to S3(JSON) and JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCOUNT_ID, HOST, USER, PASSWD, DATABASE, TABLE 정보를 입력하고 아래 코드 Cell을 복사하여 Glue Python Shell Job에 붙여넣습니다.\n",
    "#### Glue Python Shell Job을 만드는 방법은 Lab 가이드를 참고합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "import pymysql\n",
    "\n",
    "ACCOUNT_ID = ''\n",
    "HOST = ''\n",
    "USER = 'admin'\n",
    "PASSWD = ''\n",
    "DATABASE = 'analytics_hol'\n",
    "TABLE = 'titanic_train'\n",
    "\n",
    "try:\n",
    "    # Read CSV S3 file\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket='aws-glue-hol-' + ACCOUNT_ID, Key='train/titanic_train.csv')\n",
    "    data = obj['Body'].read()\n",
    "\n",
    "    # Write CSV to Buffer\n",
    "    columns = ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "    csv_dict_reader = csv.DictReader(io.BytesIO(data), columns)\n",
    "    next(csv_dict_reader, None)\n",
    "    json_buffer = io.BytesIO()\n",
    "\n",
    "    for row in csv_dict_reader:\n",
    "        json.dump(row, json_buffer)\n",
    "        json_buffer.write('\\n')\n",
    "\n",
    "    # Write Buffer to JSON file\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object('aws-glue-hol-' + ACCOUNT_ID, 'output/titanic_train.json').put(Body=json_buffer.getvalue())\n",
    "\n",
    "    # Write CSV to JDBC\n",
    "    csv_reader = csv.reader(io.BytesIO(data), delimiter=',')\n",
    "    next(csv_reader, None)\n",
    "    conn = pymysql.connect(host=HOST, user=USER, passwd=PASSWD, db=DATABASE, connect_timeout=5)\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        for row in csv_reader:\n",
    "            for n, i in enumerate(row):\n",
    "                if i == '' and n in [1, 2, 5, 6, 7, 8, 9]:\n",
    "                    row[n] = 0\n",
    "            passengerid, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = row\n",
    "            name = name.replace('\"', \"'\")\n",
    "            query = '''\n",
    "insert into {DATABASE}.{TABLE}(passengerid, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked) \n",
    "values({passengerid}, {survived}, {pclass}, \"{name}\", '{sex}', {age}, {sibsp}, {parch}, '{ticket}', {fare}, '{cabin}', '{embarked}')\n",
    "on duplicate key update passengerid={passengerid}, survived={survived}, pclass={pclass}, name=\"{name}\", sex='{sex}', age={age}, sibsp={sibsp}, parch={parch}, ticket='{ticket}', fare={fare}, cabin='{cabin}', embarked='{embarked}'\n",
    "            '''.format(\n",
    "                DATABASE=DATABASE,\n",
    "                TABLE=TABLE,\n",
    "                passengerid=passengerid, \n",
    "                survived=survived, \n",
    "                pclass=pclass, \n",
    "                name=name, \n",
    "                sex=sex, \n",
    "                age=age, \n",
    "                sibsp=sibsp, \n",
    "                parch=parch, \n",
    "                ticket=ticket, \n",
    "                fare=fare, \n",
    "                cabin=cabin, \n",
    "                embarked=embarked)\n",
    "            cursor.execute(query)\n",
    "        conn.commit()\n",
    "except Exception as e:\n",
    "    raise\n",
    "finally:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC Write 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "\n",
    "# GlueContext 생성\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "# Read Data from Glue Catalog(JDBC)\n",
    "titanic_dyf = glueContext.create_dynamic_frame.from_catalog(database=DATABASE,\n",
    "                                                           table_name='_'.join([DATABASE, TABLE]),                           \n",
    "                                                           transformation_ctx='titanic_dyf',\n",
    "                                                           additional_options={'hashexpression': 'passengerid', \n",
    "                                                                               'hashpartitions': 5})\n",
    "\n",
    "titanic_dyf.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
