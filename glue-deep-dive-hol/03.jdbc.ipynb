{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JDBC Read & Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이번 Lab에서는 Glue Job을 이용해 Database에 접근하고 Read, Write하는 예제를 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCOUNT_ID와 RDS의 HOST, USER, PASSWD 정보를 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID = ''\n",
    "HOST = ''\n",
    "DATABASE = 'analytics_hol'\n",
    "TABLE = 'titanic_train'\n",
    "JDBC_URL = 'jdbc:mysql://{HOST}:3306/{DATABASE}'.format(HOST=HOST, DATABASE=DATABASE)\n",
    "USER = 'admin'\n",
    "PASSWD = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab을 진행하기 앞서 필요한 Database와 Table을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host=HOST, user=USER, passwd=PASSWD, connect_timeout=5)   \n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        # drop db\n",
    "        query = 'DROP DATABASE IF EXISTS {DATABASE}'.format(DATABASE=DATABASE)\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # create db\n",
    "        query = 'CREATE DATABASE IF NOT EXISTS {DATABASE}'.format(DATABASE=DATABASE)\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # create table\n",
    "        query = '''\n",
    "CREATE TABLE IF NOT EXISTS {DATABASE}.{TABLE} (\n",
    "  `passengerid` int(11) NOT NULL,\n",
    "  `survived` tinyint(1) DEFAULT NULL,\n",
    "  `pclass` tinyint(4) DEFAULT NULL,\n",
    "  `name` varchar(128) COLLATE utf8_bin DEFAULT NULL,\n",
    "  `sex` char(8) DEFAULT NULL,\n",
    "  `age` tinyint(4) DEFAULT NULL,\n",
    "  `sibsp` tinyint(4) DEFAULT NULL,\n",
    "  `parch` tinyint(4) DEFAULT NULL,\n",
    "  `ticket` varchar(32) COLLATE utf8_bin DEFAULT NULL,\n",
    "  `fare` DECIMAL(10, 6) DEFAULT NULL,\n",
    "  `cabin` varchar(32) COLLATE utf8_bin DEFAULT NULL,\n",
    "  `embarked` char(1) DEFAULT NULL,\n",
    "  `created_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "  `updated_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "  PRIMARY KEY (`passengerid`),\n",
    "  KEY `survived` (`survived`),\n",
    "  KEY `pclass` (`pclass`),\n",
    "  KEY `sex` (`sex`),\n",
    "  KEY `embarked` (`embarked`),\n",
    "  KEY `created_time` (`created_time`),\n",
    "  KEY `updated_time` (`updated_time`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin\n",
    "        '''.format(DATABASE=DATABASE, TABLE=TABLE)\n",
    "        cursor.execute(query)\n",
    "\n",
    "        conn.commit()\n",
    "except Exception as e:\n",
    "    print('[ERROR]: {}'.format(e))\n",
    "    raise\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lab Guide로 돌아가서 Glue Connection을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC Write with Glue & Spark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "\n",
    "# GlueContext 생성\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "s3_bucket = 's3://aws-glue-hol-' + ACCOUNT_ID\n",
    "\n",
    "# Read Data from Glue Catalog(S3)\n",
    "titanic_dyf = glueContext.create_dynamic_frame.from_catalog(database=DATABASE,\n",
    "                                                           table_name=TABLE,                           \n",
    "                                                           transformation_ctx='titanic_dyf')\n",
    "# Write Data into Glue Catalog(JDBC)\n",
    "datasink = glueContext.write_dynamic_frame.from_catalog(\n",
    "    frame = titanic_dyf, \n",
    "    database = DATABASE, \n",
    "    table_name = '_'.join([DATABASE, TABLE]))\n",
    "\n",
    "# Spark API\n",
    "# JDBC Connection 정보\n",
    "#connectionProperties = {    \n",
    "#    \"user\" : USER,\n",
    "#    \"password\" : PASSWD,\n",
    "#    \"driver\" : \"com.mysql.jdbc.Driver\"\n",
    "#}\n",
    "\n",
    "#titanic_dyf.toDF().write.jdbc(\n",
    "#    url=JDBC_URL, \n",
    "#    table=TABLE, \n",
    "#    mode=\"overwrite\", \n",
    "#    properties=connectionProperties\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC Read with Glue API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "\n",
    "# GlueContext 생성\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "# Read Data from Glue Catalog(JDBC)\n",
    "titanic_dyf = glueContext.create_dynamic_frame.from_catalog(database=DATABASE,\n",
    "                                                           table_name='_'.join([DATABASE, TABLE]),                           \n",
    "                                                           transformation_ctx='titanic_dyf',\n",
    "                                                           additional_options={'hashexpression': 'passengerid', \n",
    "                                                                               'hashpartitions': 5})\n",
    "\n",
    "titanic_dyf.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC Read with Spark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "\n",
    "# JDBC 접속 정보\n",
    "connectionProperties = {\n",
    "    \"user\" : USER,\n",
    "    \"password\" : PASSWD,\n",
    "    \"driver\" : \"com.mysql.jdbc.Driver\",\n",
    "    \"fetchsize\" : \"1000\"\n",
    "}\n",
    "\n",
    "# pushdown_query를 이용해 부분 데이터만 읽어올 수 있다.\n",
    "pushdown_query = \"(select * from {TABLE} where passengerid < 100) {TABLE}_alias\".format(TABLE=TABLE)\n",
    "\n",
    "# lowerBound, upperBound, numPartitions은 Partition을 나누는 기준을 정할 때 사용되며 데이터를 읽어오는 범위와는 관계없음\n",
    "titanic_df = spark.read.jdbc(\n",
    "        url=JDBC_URL,\n",
    "        table=pushdown_query,\n",
    "        column=\"passengerid\",\n",
    "        lowerBound=1,\n",
    "        upperBound=100,\n",
    "        numPartitions=5,\n",
    "        properties=connectionProperties)\n",
    "\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
