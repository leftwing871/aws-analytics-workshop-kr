{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glue Job Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이번 Lab에서는 SageMaker notebook을 사용하여 간단하게 데이터를 읽고, ETL하고, Write하는 과정을 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3에 업로드한 데이터를 읽어오기 위해 각자 S3 bucket에 지정한 account-id를 account_id 변수에 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID = ''\n",
    "S3_BUCKET = 's3://aws-glue-hol-' + ACCOUNT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook에서는 SparkContext(sc)와 SparkSession(spark)이 지원되므로 선언해서 사용하지 않아도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook에서는 아래와 같이 이미 선언된 SparkContext(sc)를 사용해서 GlueContext 생성이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Jupyter Notebook에서 이미 생성한 SparkContext(sc)를 사용해서 GlueContext 변수 생성\n",
    "glueContext = GlueContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 데이터를 읽어 DynamicFrame 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dyf = glueContext.create_dynamic_frame_from_options(\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths': [S3_BUCKET + '/train']},\n",
    "    format='csv',\n",
    "    format_options={\n",
    "        'withHeader': True,\n",
    "        'delimiter': ','\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DynamicFrame을 DataFrame으로 변환하고 정상적으로 변환되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df = titanic_dyf.toDF()\n",
    "titanic_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 데이터 수를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark API를 이용하여 Name 컬럼에서 유의미한 String만 추출하여 initial 컬럼을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_df = titanic_csv_df.withColumn('initial', regexp_extract(col('Name'), \"(\\w+)\\.\", 1))\n",
    "titanic_csv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glue API를 이용하여 사용하지 않는 Name 컬럼은 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "titanic_csv_dyf = DynamicFrame.fromDF(titanic_csv_df, glueContext, 'titanic_csv_dyf').drop_fields('Name')\n",
    "titanic_csv_dyf.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Json format으로 output 디렉토리에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_csv_dyf.toDF().write \\\n",
    "    .format('json') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save(S3_BUCKET + '/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Json으로 변환해서 저장된 데이터가 정상적으로 저장되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_json_df = glueContext.create_dynamic_frame_from_options(\n",
    "    connection_type = 's3',\n",
    "    connection_options = {'paths': [S3_BUCKET + '/output']},\n",
    "    format='json').toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_json_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
